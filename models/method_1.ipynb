{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here I translate GCN in tensorflow to GCN in tensorflow.keras\n",
    "### This is the current method i'm focusing on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.0-rc2\n",
      "2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "# tf and tf keras versions\n",
    "print(tf.VERSION)\n",
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods are functions associated with a class\n",
    "# the data are the attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = np.load('/Users/b_eebs/tf-keras/ZINC/adj/0.npy')\n",
    "features = np.load('/Users/b_eebs/tf-keras/ZINC/features/0.npy')\n",
    "test = (np.load('/Users/b_eebs/tf-keras/ZINC/logP.npy')[0:10000]).astype(float) #takes the first 10,000 molecules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class allows us to build the weight and biases for each convolutional layer and also each model. After building the biases in the build method, I will perform the matrix multiplications in the call method for each model. The cell below will iteratively call upon all layers, until the whole model is build. The model will then be compiled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "import conv_models\n",
    "\n",
    "class MyLayer(layers.Layer):\n",
    "    models = ['gcn', 'gcn_a', 'gcn_g', 'gcn_ag']\n",
    "    \n",
    "    def __init__(self, output_dim, adjacency, features, \n",
    "                 weight_shape, bias_shape, model, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        self.features = tf.placeholder(shape = [100,50,58], \n",
    "                                       dtype = 'float64')\n",
    "        self.adjacency = tf.placeholder(shape = [100,50,50], \n",
    "                                        dtype = 'float64')\n",
    "        self.weight_shape = weight_shape\n",
    "        self.bias_shape = bias_shape\n",
    "        self.model = model\n",
    "        self.kernel = None\n",
    "        self.bias = None\n",
    "        super(MyLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    # still have to add attention weights\n",
    "    # initializers may be incorrect\n",
    "    def build(self):\n",
    "        #self.weight_shape = weight_shape\n",
    "        #self.bias_shape = bias_shape\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                  shape=self.weight_shape,\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True,dtype='float64')\n",
    "        \n",
    "        self.bias = self.add_weight(name='bias',\n",
    "                                shape=self.bias_shape,\n",
    "                                initializer='glorot_uniform',\n",
    "                                trainable=True,dtype='float64')\n",
    "        print(self.bias,self.kernel)\n",
    "        \n",
    "        #why did I have to get rid of this line?\n",
    "        #super(MyLayer, self).build()\n",
    "    \n",
    "    #here is where I will implement all models, which will be built in another script. \n",
    "    def call(self):\n",
    "        if self.model == MyLayer.models[0]:\n",
    "            _features = conv_models.GCN(self.features,self.adjacency,\n",
    "                                       self.kernel,self.bias)\n",
    "        elif self.model == MyLayer.models[1]:\n",
    "            print('go to gcn+att function and calc. mult.')\n",
    "        elif self.model == MyLayer.models[2]:\n",
    "            print('go to gcn+gate function and calc. mult.')\n",
    "        elif self.model == MyLayer.models[3]:\n",
    "            print('go to aug. gcn function and calc. mult.')\n",
    "        else:\n",
    "            raise ValueError('not a valid model')\n",
    "        return(_features)\n",
    "#look into the following functions, especially the last\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        shape = tf.TensorShape(input_shape).as_list()\n",
    "        shape[-1] = self.output_dim\n",
    "        return tf.TensorShape(shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super(MyLayer, self).get_config()\n",
    "        base_config['output_dim'] = self.output_dim\n",
    "        return base_config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "<tf.Variable 'bias:0' shape=(32,) dtype=float64> <tf.Variable 'kernel:0' shape=(58, 32) dtype=float64>\n",
      "WARNING:tensorflow:From /Users/b_eebs/tf-keras/models/conv_models.py:6: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "the shape Tensor(\"Relu:0\", shape=(100, 50, 32), dtype=float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu:0' shape=(100, 50, 32) dtype=float64>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is where all models will be built\n",
    "\n",
    "#for now i've constructed the input layer whose output shape matches the output shape of Ryu et al's input layer\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "num_atoms = int(features.shape[1])\n",
    "input_dim = int(features.shape[2])\n",
    "hidden_dim = [input_dim]\n",
    "num_layers=3\n",
    "\n",
    "input_layer = MyLayer(32,adj,features,[input_dim,32],[32],'gcn')\n",
    "input_layer.build() # get weights and bias\n",
    "input_layer.call() # propogation rule argument\n",
    "# now we need to relu, and repeat for L hidden layer times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
