{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# implement normalization\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from tensorflow.python.keras.callbacks import TensorBoard \n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(layers.Layer):\n",
    "    \"\"\"\n",
    "    A graph convolution model\n",
    "    \"\"\"\n",
    "    tf.reset_default_graph()\n",
    "    def __init__(self, output_dim,  **kwargs): # comment out inputs=[x, y],\n",
    "        self.output_dim = output_dim\n",
    "        super(GCN, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # print(input_shape[0][2])\n",
    "        shape = tf.TensorShape((input_shape[0][2], self.output_dim))\n",
    "        shape = [int(shape[0]),int(shape[1])] # [50 , 32]\n",
    "        \n",
    "        with K.name_scope('Parameters'):\n",
    "            with K.name_scope('Weight_' + str(shape[1])):\n",
    "                self.kernel = self.add_weight(name='conv_weight',\n",
    "                                      shape=shape,\n",
    "                                      initializer='glorot_uniform',\n",
    "                                      trainable=True,dtype=tf.float32)\n",
    "            with K.name_scope('Bias_'+str(shape[1])):\n",
    "                self.bias = self.add_weight(name='conv_bias',\n",
    "                                    shape=[shape[1]],\n",
    "                                    initializer='glorot_uniform',\n",
    "                                    trainable=True,dtype=tf.float32)\n",
    "\n",
    "    def call(self, input):\n",
    "        X, A = input[0], input[1]\n",
    "        dim = self.kernel.get_shape()[1]\n",
    "        num_atoms = A.get_shape()[1]\n",
    "        with K.name_scope('Prop.Rule_'+ str(X.get_shape()[2])):\n",
    "            _b = tf.reshape(tf.tile(self.bias, [num_atoms]), [num_atoms, dim])\n",
    "            _X = tf.einsum('ijk,kl->ijl', X, self.kernel) + _b\n",
    "        #_X = get_skip_connection(_X, X)\n",
    "            return _X\n",
    "\n",
    "class G2N(layers.Layer):\n",
    "    \"\"\"\n",
    "    A layer to sum the node feature to preserve the permutation invariance\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dim,  **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(G2N, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        shape = tf.TensorShape((input_shape[2], self.output_dim))\n",
    "        shape = [int(shape[0]),int(shape[1])] # [50 , 32]\n",
    "        \n",
    "        with K.name_scope('Parameters'):\n",
    "            with K.name_scope('Weight'+str(shape[1])):\n",
    "                self.kernel = self.add_weight(name='permu_weight',\n",
    "                                          shape=shape,\n",
    "                                          initializer='glorot_uniform',\n",
    "                                          trainable=True,dtype=tf.float32)\n",
    "\n",
    "    def call(self, X):\n",
    "        #print(X.get_shape())\n",
    "        #print(self.kernel.get_shape())\n",
    "        with K.name_scope('ATOMWISE'):\n",
    "            Z = tf.einsum('ijk,kl->ijl', X, self.kernel)\n",
    "            Z = tf.nn.relu(Z)\n",
    "            Z = tf.nn.sigmoid(tf.reduce_sum(Z, 1))\n",
    "            return Z\n",
    "\n",
    "class mymodel(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    A generallized model for the prediction of properties from graph\n",
    "    \"\"\"\n",
    "    def __init__(self):#, inputs=[x, y]):\n",
    "        \n",
    "        super(mymodel, self).__init__(name='')\n",
    "        self.gcn1 = GCN(32) #, input_shape=[58]\n",
    "        #self.gcn2 = GCN(32) #, input_shape=[58])\n",
    "        self.gcn3 = GCN(32) #, input_shape=[58])\n",
    "        self.g2n = G2N(64)#, input_shape=[58])\n",
    "        self.dense1 = layers.Dense(64, activation=tf.nn.relu, input_shape=[64])\n",
    "        self.dense2 = layers.Dense(64, activation=tf.nn.relu, input_shape=[64])\n",
    "        self.dense3 = layers.Dense(1)\n",
    "\n",
    "    def call(self, input):\n",
    "        X, A = input[0], input[1]\n",
    "        print(X,A)\n",
    "        with K.name_scope('graph_conv-1.{}'.format(X.get_shape()[2])):\n",
    "            x = tf.nn.relu(self.gcn1([X, A]))\n",
    "        with K.name_scope('graph_conv-2.{}'.format(x.get_shape()[2])):\n",
    "            #x = tf.nn.relu(self.gcn2(x))\n",
    "            x = tf.nn.relu(self.gcn3([x, A]))\n",
    "        with K.name_scope('Perm-Invariance'):\n",
    "            x = tf.nn.relu(self.g2n(x))\n",
    "        with K.name_scope('Latent_Space-64'):\n",
    "            x = self.dense1(x)\n",
    "        # molecule_embedding1 = tf.get_variable('embedding 1',[32, 64])\n",
    "        # embedded_molecules = [tf.nn.embedding_lookup(molecule_embedding1, x)]\n",
    "        with K.name_scope('Latent_Space-64'):\n",
    "            x = self.dense2(x)\n",
    "        # molecule_embedding2 = tf.get_variable('embedding 2',[64, 64])\n",
    "        # embedded_molecules.append(tf.nn.embedded_molecules(molecule_embedding2,x))\n",
    "        with K.name_scope('Output'):\n",
    "            x = self.dense3(x)\n",
    "        # molecule_embedding3 = tf.get_variable('embedding1',[64, 1])\n",
    "        # embedded_molecules = tf.nn.embedding_lookup(molecule_embedding3,[100])\n",
    "        \n",
    "        return x    \n",
    "\n",
    "def load_data(path=\"/Users/b_eebs/tf-keras/data/\", ids=10000): \n",
    "    \"\"\"\n",
    "    load the source data from somewhere\n",
    "    \"\"\"\n",
    "    features = np.load(path+'fea.npy')[:ids] \n",
    "    adj = np.load(path+'adj.npy')[:ids]\n",
    "    prop = np.load(path+'prop.npy')[:ids]\n",
    "    #features = np.reshape(features, [ids, features.shape[1]*features.shape[2]])\n",
    "    return adj, features, prop\n",
    "\n",
    "class Progress(keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    A simple function to show the progress\n",
    "    \"\"\"\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        if epoch % 10 == 0:\n",
    "             print('epoch-----', epoch, logs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instead of writing script to plot specified values, we can use tensorboard to plot several parameters with an addition of a couple of lines.\n",
    "\n",
    "## We can also plot the bias and weights along with their gradients.\n",
    "\n",
    "## Embeddings are still in progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to do an embedding, nice to include embedding data, still looking into this.\n",
    "# - load in a metadata set from another .npy file\n",
    "\n",
    "## normally you have to restart the whole notebook before running, or find a way to dump all the embeddings. \n",
    "## K.clear_session() doesn't always work. Furthermore, if you want to re-reun the EXACT SAME training process, \n",
    "## delete your previous log file first. The results won't overwrite automatically.\n",
    "\n",
    "adj, features, props = load_data() # before running you may have to change load_data path\n",
    "print('shape of feature: ', np.shape(features))\n",
    "model = mymodel()\n",
    "lr = 0.001\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"./summaries/ADAM-0.001\",histogram_freq=10,batch_size=100, \n",
    "                          write_graph=True, write_grads=True, update_freq='batch')\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr)\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['mean_absolute_error', 'mean_squared_error'])\n",
    "\n",
    "input = {'input_1': features, 'input_2': adj}\n",
    "\n",
    "history = model.fit(x=[features, adj], y=props, batch_size=100, \n",
    "                    epochs=1000, validation_split=0.1, verbose=0,\n",
    "                    callbacks=[tensorboard])\n",
    "model.summary()\n",
    "\n",
    "K.clear_session()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------to test other optimizers--------------------------------------\n",
    "## restart the whole kernel before running this cell\n",
    "\n",
    "adj, features, props = load_data() # before running you may have to change load_data path\n",
    "\n",
    "model = mymodel()\n",
    "lr = 0.001\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"./summaries/ADAMMAX-0.001\",histogram_freq=10,batch_size=100, \n",
    "                          write_graph=True, write_grads=True, update_freq='batch')\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adamax(lr)\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['mean_absolute_error', 'mean_squared_error'])\n",
    "input = {'input_1': features, 'input_2': adj}\n",
    "history = model.fit(x=[features, adj], y=props, batch_size=100, \n",
    "                    epochs=1000, validation_split=0.1, verbose=0,\n",
    "                    callbacks=[tensorboard])\n",
    "\n",
    "\n",
    "\n",
    "model.summary()\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "\n",
    "adj, features, props = load_data() # before running you may have to change load_data path\n",
    "\n",
    "model = mymodel()\n",
    "lr = 0.001\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"./summaries/SGD-0.001\",histogram_freq=10,batch_size=100, \n",
    "                          write_graph=True, write_grads=True, update_freq='batch')\n",
    "optimizer = tf.keras.optimizers.Adam(lr)\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['mean_absolute_error', 'mean_squared_error'])\n",
    "input = {'input_1': features, 'input_2': adj}\n",
    "history = model.fit(x=[features, adj], y=props, batch_size=100, \n",
    "                    epochs=1000, validation_split=0.1, verbose=0,\n",
    "                    callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Tensor(\"input_1:0\", shape=(?, 50, 58), dtype=float32) Tensor(\"input_2:0\", shape=(?, 50, 50), dtype=float32)\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gcn_2 (GCN)                  multiple                  1888      \n",
      "_________________________________________________________________\n",
      "gcn_3 (GCN)                  multiple                  1056      \n",
      "_________________________________________________________________\n",
      "g2n_1 (G2N)                  multiple                  2048      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              multiple                  65        \n",
      "=================================================================\n",
      "Total params: 13,377\n",
      "Trainable params: 13,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## restart kernel\n",
    "\n",
    "adj, features, props = load_data() # before running you may have to change load_data path\n",
    "\n",
    "model = mymodel()\n",
    "lr = 0.001\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"./summaries/RMS-0.001\",histogram_freq=10,batch_size=100, \n",
    "                          write_graph=True, write_grads=True, update_freq='batch')\n",
    "optimizer = tf.keras.optimizers.RMSprop(lr)\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['mean_absolute_error', 'mean_squared_error'])\n",
    "input = {'input_1': features, 'input_2': adj}\n",
    "history = model.fit(x=[features, adj], y=props, batch_size=100, \n",
    "                    epochs=1000, validation_split=0.1, verbose=0,\n",
    "                    callbacks=[tensorboard])\n",
    "model.summary()\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To do:\n",
    "### - interpret plots\n",
    "### - embedding\n",
    "### - augmented gcn tensorboard\n",
    "### - look at geometric torch\n",
    "### - look at our pytorch implementation\n",
    "### - undergrad research conference abstract \n",
    "### - look into runtime metadata!\n",
    "\n",
    "https://jhui.github.io/2017/03/12/TensorBoard-visualize-your-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
